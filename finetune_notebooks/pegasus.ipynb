{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_file, max_input_length, max_target_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_text = self.data.iloc[index][\"text\"]\n",
    "        target_text = self.data.iloc[index][\"titles\"]\n",
    "\n",
    "        # Tokenize the input and target texts\n",
    "        inputs = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=self.max_input_length, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=self.max_target_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids[0],\n",
    "            \"attention_mask\": inputs.attention_mask[0],\n",
    "            \"labels\": targets.input_ids[0],\n",
    "        }\n",
    "\n",
    "# Check if CUDA is available and use the GPU if so\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and the Pegasus model pretrained on the GPU\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model_pegasus = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(device)\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = CustomDataset(tokenizer, \"data/train.csv\", max_input_length=128, max_target_length=128)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus_trained_model\",\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per GPU\n",
    "    save_steps=100,  # Model saving frequency\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define the Trainer for training\n",
    "trainer = Trainer(\n",
    "    model=model_pegasus,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "model_pegasus.save_pretrained(\"trained_pegasus_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate summaries for the test data\n",
    "batch_size = 4  # Adjust batch size as needed\n",
    "test_data = pd.read_csv(\"data/test_text.csv\")\n",
    "num_batches = (len(test_data) + batch_size - 1) // batch_size\n",
    "\n",
    "generated_summaries = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_pegasus.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_batches):\n",
    "        batch_data = test_data.iloc[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        input_encodings = tokenizer(batch_data[\"text\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate summaries\n",
    "        outputs = model_pegasus.generate(input_encodings[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "        # Decode the generated summaries\n",
    "        decoded_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        generated_summaries.extend(decoded_summaries)\n",
    "\n",
    "# Create a DataFrame for the submission\n",
    "submission_df = pd.DataFrame({\"ID\": test_data[\"ID\"], \"titles\": generated_summaries})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission_df.to_csv(\"pegasus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
