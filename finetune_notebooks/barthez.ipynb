{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install datasets rouge_score nltk\n","# !pip install accelerate -U\n","# !pip install transformers==4.27.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:26:24.960871Z","iopub.status.busy":"2024-03-25T14:26:24.960593Z","iopub.status.idle":"2024-03-25T14:27:01.904236Z","shell.execute_reply":"2024-03-25T14:27:01.903378Z","shell.execute_reply.started":"2024-03-25T14:26:24.960845Z"},"trusted":true},"outputs":[],"source":["# Transformers\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM     # BERT Tokenizer and architecture\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model\n","from transformers import pipeline                                         # Pipeline\n","from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data \n","import string                                                              # PyTorch\n","import pandas as pd\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","from datasets import load_metric, Dataset\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-25T14:27:01.906594Z","iopub.status.busy":"2024-03-25T14:27:01.905979Z","iopub.status.idle":"2024-03-25T14:27:04.162688Z","shell.execute_reply":"2024-03-25T14:27:04.161768Z","shell.execute_reply.started":"2024-03-25T14:27:01.906563Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv('train.csv')\n","test_data = pd.read_csv('test_text.csv')\n","val_data = pd.read_csv('validation.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:04.164805Z","iopub.status.busy":"2024-03-25T14:27:04.164378Z","iopub.status.idle":"2024-03-25T14:27:04.171364Z","shell.execute_reply":"2024-03-25T14:27:04.170381Z","shell.execute_reply.started":"2024-03-25T14:27:04.164766Z"},"trusted":true},"outputs":[],"source":["def clean_text(text):\n","        text = text.lower()\n","        text = re.sub('^.*?- ', '', text)\n","        text = re.sub('\\[.*?\\]', '', text)\n","        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","        text = re.sub('<.*?>+', '', text)\n","        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","        text = re.sub('\\n', '', text)\n","        text = re.sub('\\w*\\d\\w*', '', text)\n","        return text\n","\n","\n","def clean_text_source(text):\n","        text = text.lower()\n","        text = re.sub('^.*?- ', '', text)\n","        text = re.sub('\\[.*?\\]', '', text)\n","        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","        text = re.sub('<.*?>+', '', text)\n","        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","        text = re.sub('\\n', '', text)\n","        text = re.sub('\\w*\\d\\w*', '', text)\n","        return 'summarize: ' + text\n","\n","\n","def clean_df(df, cols):\n","    for col in cols:\n","        if col == 'text':\n","            df[col] = df[col].fillna('').apply(clean_text_source)\n","        else:\n","            df[col] = df[col].fillna('').apply(clean_text)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:04.174047Z","iopub.status.busy":"2024-03-25T14:27:04.173686Z","iopub.status.idle":"2024-03-25T14:27:04.642298Z","shell.execute_reply":"2024-03-25T14:27:04.641482Z","shell.execute_reply.started":"2024-03-25T14:27:04.174021Z"},"trusted":true},"outputs":[],"source":["train_data = clean_df(train_data,['text', 'titles'])\n","test_data = clean_df(test_data,['text'])\n","val_data = clean_df(val_data,['text', 'titles'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:04.643684Z","iopub.status.busy":"2024-03-25T14:27:04.643373Z","iopub.status.idle":"2024-03-25T14:27:04.911983Z","shell.execute_reply":"2024-03-25T14:27:04.910974Z","shell.execute_reply.started":"2024-03-25T14:27:04.643657Z"},"trusted":true},"outputs":[],"source":["train_ds = Dataset.from_pandas(train_data)\n","test_ds = Dataset.from_pandas(test_data)\n","val_ds = Dataset.from_pandas(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:04.913486Z","iopub.status.busy":"2024-03-25T14:27:04.913215Z","iopub.status.idle":"2024-03-25T14:27:18.384144Z","shell.execute_reply":"2024-03-25T14:27:18.382971Z","shell.execute_reply.started":"2024-03-25T14:27:04.913462Z"},"trusted":true},"outputs":[],"source":["checkpoint = 'moussaKam/barthez' # Model\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint) # Loading Tokenizer\n","model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:18.388200Z","iopub.status.busy":"2024-03-25T14:27:18.385442Z","iopub.status.idle":"2024-03-25T14:27:18.394430Z","shell.execute_reply":"2024-03-25T14:27:18.393502Z","shell.execute_reply.started":"2024-03-25T14:27:18.388172Z"},"trusted":true},"outputs":[],"source":["def preprocess_function(examples):\n","    inputs = [doc for doc in examples[\"text\"]]\n","    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"titles\"], max_length=128, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:27:18.395988Z","iopub.status.busy":"2024-03-25T14:27:18.395689Z","iopub.status.idle":"2024-03-25T14:29:33.484487Z","shell.execute_reply":"2024-03-25T14:29:33.483417Z","shell.execute_reply.started":"2024-03-25T14:27:18.395957Z"},"trusted":true},"outputs":[],"source":["# Applying preprocess_function to the datasets\n","tokenized_train = train_ds.map(preprocess_function, batched=True,\n","                               remove_columns=['text', 'titles']) # Removing features\n","# Removing features\n","tokenized_val = val_ds.map(preprocess_function, batched=True,\n","                               remove_columns=['text', 'titles']) # Removing features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:33.486402Z","iopub.status.busy":"2024-03-25T14:29:33.485949Z","iopub.status.idle":"2024-03-25T14:29:33.491198Z","shell.execute_reply":"2024-03-25T14:29:33.490313Z","shell.execute_reply.started":"2024-03-25T14:29:33.486353Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:51.045344Z","iopub.status.busy":"2024-03-25T14:29:51.045020Z","iopub.status.idle":"2024-03-25T14:29:52.411952Z","shell.execute_reply":"2024-03-25T14:29:52.410916Z","shell.execute_reply.started":"2024-03-25T14:29:51.045313Z"},"trusted":true},"outputs":[],"source":["metric = load_metric('rouge')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:52.413619Z","iopub.status.busy":"2024-03-25T14:29:52.413299Z","iopub.status.idle":"2024-03-25T14:29:52.424139Z","shell.execute_reply":"2024-03-25T14:29:52.423116Z","shell.execute_reply.started":"2024-03-25T14:29:52.413592Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred# Obtaining predictions and true labels\n","    \n","    # Decoding predictions\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    \n","    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label = -100)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip(), language='french')) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip(), language='french')) for label in decoded_labels]\n","    \n","    \n","    # Computing rouge score\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extracting some results\n","\n","    # Add mean-generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:52.425836Z","iopub.status.busy":"2024-03-25T14:29:52.425512Z","iopub.status.idle":"2024-03-25T14:29:52.487333Z","shell.execute_reply":"2024-03-25T14:29:52.486404Z","shell.execute_reply.started":"2024-03-25T14:29:52.425809Z"},"trusted":true},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir = 'barthez',\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = 'epoch',\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'eval_loss',\n","    seed = 8,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=2,\n","    weight_decay=0.01,\n","    save_total_limit=4,\n","    num_train_epochs=16,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to=\"none\"\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:52.488732Z","iopub.status.busy":"2024-03-25T14:29:52.488423Z","iopub.status.idle":"2024-03-25T14:36:52.736605Z","shell.execute_reply":"2024-03-25T14:36:52.735236Z","shell.execute_reply.started":"2024-03-25T14:29:52.488706Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get all the directories in 'barthez' directory\n","import os\n","dirs = os.listdir('barthez')\n","tokenizer_kwargs = {'max_length':1024}\n","\n","for dir in dirs:\n","    pipeline_sum = pipeline('summarization', model=f'barthez/{dir}', device=0 )\n","    test_data['titles'] = test_data['text'].apply(lambda x: pipeline_sum(x, **tokenizer_kwargs)[0]['summary_text'])\n","    test_data.drop('text', axis=1, inplace=True)\n","    test_data.to_csv(f'{dir}_test.csv', index=False)\n","    print(f'{dir} done')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4616152,"sourceId":7867765,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
