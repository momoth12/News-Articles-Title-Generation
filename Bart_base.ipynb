{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from torch import optim\n",
    "\n",
    "# Load the data from the CSV file\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "val_data = pd.read_csv(\"data/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns containing the texts and summaries\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_summaries = train_data[\"titles\"].tolist()\n",
    "\n",
    "val_texts = val_data[\"text\"].tolist()\n",
    "val_summaries = val_data[\"titles\"].tolist()\n",
    "\n",
    "# Load the tokenizer and the Bart model for text generation\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\mouha/.cache\\huggingface\\transformers\\486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 12.666672706604004\n",
      "Epoch 0, Step 100, Loss: 1.2392264604568481\n",
      "Epoch 0, Step 200, Loss: 0.3654614984989166\n",
      "Epoch 0, Step 300, Loss: 1.3258610963821411\n",
      "Epoch 0, Step 400, Loss: 0.7161915302276611\n",
      "Epoch 0, Step 500, Loss: 1.114119529724121\n",
      "Epoch 0, Step 600, Loss: 0.845808744430542\n",
      "Epoch 0, Step 700, Loss: 0.9309849143028259\n",
      "Epoch 0, Step 800, Loss: 2.3252220153808594\n",
      "Epoch 0, Step 900, Loss: 0.4593234360218048\n",
      "Epoch 0, Step 1000, Loss: 1.6621222496032715\n",
      "Epoch 0, Step 1100, Loss: 1.4235011339187622\n",
      "Epoch 0, Step 1200, Loss: 2.5234172344207764\n",
      "Epoch 0, Step 1300, Loss: 1.4325523376464844\n",
      "Epoch 0, Step 1400, Loss: 0.8728844523429871\n",
      "Epoch 0, Step 1500, Loss: 1.5192229747772217\n",
      "Epoch 0, Step 1600, Loss: 1.562177300453186\n",
      "Epoch 0, Step 1700, Loss: 0.6252636313438416\n",
      "Epoch 0, Step 1800, Loss: 0.6986242532730103\n",
      "Epoch 0, Step 1900, Loss: 1.255396842956543\n",
      "Epoch 0, Step 2000, Loss: 1.1824023723602295\n",
      "Epoch 0, Step 2100, Loss: 1.311476230621338\n",
      "Epoch 0, Step 2200, Loss: 1.8002007007598877\n",
      "Epoch 0, Step 2300, Loss: 1.5797486305236816\n",
      "Epoch 0, Step 2400, Loss: 1.5284497737884521\n",
      "Epoch 0, Step 2500, Loss: 1.682240605354309\n",
      "Epoch 0, Step 2600, Loss: 0.8423565626144409\n",
      "Epoch 0, Step 2700, Loss: 0.995194673538208\n",
      "Epoch 0, Step 2800, Loss: 1.189770221710205\n",
      "Epoch 0, Step 2900, Loss: 1.2365286350250244\n",
      "Epoch 0, Step 3000, Loss: 1.3158581256866455\n",
      "Epoch 0, Step 3100, Loss: 1.3569532632827759\n",
      "Epoch 0, Step 3200, Loss: 1.2301994562149048\n",
      "Epoch 0, Step 3300, Loss: 0.3381739854812622\n",
      "Epoch 0, Step 3400, Loss: 1.6478272676467896\n",
      "Epoch 0, Step 3500, Loss: 1.1328853368759155\n",
      "Epoch 0, Step 3600, Loss: 2.4725935459136963\n",
      "Epoch 0, Step 3700, Loss: 0.984505832195282\n",
      "Epoch 0, Step 3800, Loss: 0.5303091406822205\n",
      "Epoch 0, Step 3900, Loss: 0.6452982425689697\n",
      "Epoch 0, Step 4000, Loss: 1.8050540685653687\n",
      "Epoch 0, Step 4100, Loss: 1.36684250831604\n",
      "Epoch 0, Step 4200, Loss: 0.18023966252803802\n",
      "Epoch 0, Step 4300, Loss: 1.2166647911071777\n",
      "Epoch 0, Step 4400, Loss: 1.0989032983779907\n",
      "Epoch 0, Step 4500, Loss: 0.8454029560089111\n",
      "Epoch 0, Step 4600, Loss: 0.6443449258804321\n",
      "Epoch 0, Step 4700, Loss: 0.3174717426300049\n",
      "Epoch 0, Step 4800, Loss: 0.5668708086013794\n",
      "Epoch 0, Step 4900, Loss: 0.7567415237426758\n",
      "Epoch 0, Step 5000, Loss: 1.7324362993240356\n",
      "Epoch 0, Step 5100, Loss: 0.6656569242477417\n",
      "Epoch 0, Step 5200, Loss: 0.84035724401474\n",
      "Epoch 0, Step 5300, Loss: 2.0333762168884277\n",
      "Epoch 0, Step 5400, Loss: 0.5750802159309387\n",
      "Epoch 0, Step 5500, Loss: 1.5026241540908813\n",
      "Epoch 0, Step 5600, Loss: 0.976309597492218\n",
      "Epoch 0, Step 5700, Loss: 0.22095952928066254\n",
      "Epoch 0, Step 5800, Loss: 2.3232574462890625\n",
      "Epoch 0, Step 5900, Loss: 0.3117695450782776\n",
      "Epoch 0, Step 6000, Loss: 0.7278514504432678\n",
      "Epoch 0, Step 6100, Loss: 0.5747121572494507\n",
      "Epoch 0, Step 6200, Loss: 1.168972373008728\n",
      "Epoch 0, Step 6300, Loss: 0.635735809803009\n",
      "Epoch 0, Step 6400, Loss: 2.156313419342041\n",
      "Epoch 0, Step 6500, Loss: 1.250801920890808\n",
      "Epoch 0, Step 6600, Loss: 0.7355774641036987\n",
      "Epoch 0, Step 6700, Loss: 1.650810718536377\n",
      "Epoch 0, Step 6800, Loss: 0.5592581629753113\n",
      "Epoch 0, Step 6900, Loss: 2.3920507431030273\n",
      "Epoch 0, Step 7000, Loss: 2.0052380561828613\n",
      "Epoch 0, Step 7100, Loss: 0.376498281955719\n",
      "Epoch 0, Step 7200, Loss: 2.323687791824341\n",
      "Epoch 0, Step 7300, Loss: 0.14299659430980682\n",
      "Epoch 0, Step 7400, Loss: 0.9769454002380371\n",
      "Epoch 0, Step 7500, Loss: 2.169823169708252\n",
      "Epoch 0, Step 7600, Loss: 0.3025791645050049\n",
      "Epoch 0, Step 7700, Loss: 1.8727604150772095\n",
      "Epoch 0, Step 7800, Loss: 0.6275286674499512\n",
      "Epoch 0, Step 7900, Loss: 1.0668373107910156\n",
      "Epoch 0, Step 8000, Loss: 1.2001146078109741\n",
      "Epoch 0, Step 8100, Loss: 1.0756397247314453\n",
      "Epoch 0, Step 8200, Loss: 0.3895631432533264\n",
      "Epoch 0, Step 8300, Loss: 0.3551403284072876\n",
      "Epoch 0, Step 8400, Loss: 1.2885453701019287\n",
      "Epoch 0, Step 8500, Loss: 1.8627440929412842\n",
      "Epoch 0, Step 8600, Loss: 0.5427480936050415\n",
      "Epoch 0, Step 8700, Loss: 1.8551465272903442\n",
      "Epoch 0, Step 8800, Loss: 0.5291655659675598\n",
      "Epoch 0, Step 8900, Loss: 0.9192236661911011\n",
      "Epoch 0, Step 9000, Loss: 1.9848532676696777\n",
      "Epoch 0, Step 9100, Loss: 2.2706286907196045\n",
      "Epoch 0, Step 9200, Loss: 1.1004538536071777\n",
      "Epoch 0, Step 9300, Loss: 1.9995912313461304\n",
      "Epoch 0, Step 9400, Loss: 0.3874692916870117\n",
      "Epoch 0, Step 9500, Loss: 1.276389241218567\n",
      "Epoch 0, Step 9600, Loss: 1.359702229499817\n",
      "Epoch 0, Step 9700, Loss: 1.0258188247680664\n",
      "Epoch 0, Step 9800, Loss: 0.8687513470649719\n",
      "Epoch 0, Step 9900, Loss: 0.39392054080963135\n",
      "Epoch 0, Step 10000, Loss: 0.7745777368545532\n",
      "Epoch 0, Step 10100, Loss: 0.7707798480987549\n",
      "Epoch 0, Step 10200, Loss: 0.3208119869232178\n",
      "Epoch 0, Step 10300, Loss: 1.324536681175232\n",
      "Epoch 0, Step 10400, Loss: 0.8034078478813171\n",
      "Epoch 0, Step 10500, Loss: 0.6030583381652832\n",
      "Epoch 0, Step 10600, Loss: 2.003208875656128\n",
      "Epoch 0, Step 10700, Loss: 0.6764132380485535\n",
      "Epoch 0, Step 10800, Loss: 0.6380314230918884\n",
      "Epoch 0, Step 10900, Loss: 0.7082909941673279\n",
      "Epoch 0, Step 11000, Loss: 0.33353862166404724\n",
      "Epoch 0, Step 11100, Loss: 0.8095717430114746\n",
      "Epoch 0, Step 11200, Loss: 1.887568712234497\n",
      "Epoch 0, Step 11300, Loss: 1.1236457824707031\n",
      "Epoch 0, Step 11400, Loss: 0.6116474270820618\n",
      "Epoch 0, Step 11500, Loss: 1.3085362911224365\n",
      "Epoch 0, Step 11600, Loss: 2.017559051513672\n",
      "Epoch 0, Step 11700, Loss: 0.5018548965454102\n",
      "Epoch 0, Step 11800, Loss: 0.48291903734207153\n",
      "Epoch 0, Step 11900, Loss: 2.0876238346099854\n",
      "Epoch 0, Step 12000, Loss: 1.044764757156372\n",
      "Epoch 0, Step 12100, Loss: 0.4504546523094177\n",
      "Epoch 0, Step 12200, Loss: 0.8007461428642273\n",
      "Epoch 0, Step 12300, Loss: 1.0195322036743164\n",
      "Epoch 0, Step 12400, Loss: 1.445865273475647\n",
      "Epoch 0, Step 12500, Loss: 2.081023931503296\n",
      "Epoch 0, Step 12600, Loss: 1.0127367973327637\n",
      "Epoch 0, Step 12700, Loss: 1.7835887670516968\n",
      "Epoch 0, Step 12800, Loss: 0.36290454864501953\n",
      "Epoch 0, Step 12900, Loss: 1.6333714723587036\n",
      "Epoch 0, Step 13000, Loss: 1.2104525566101074\n",
      "Epoch 0, Step 13100, Loss: 1.10549795627594\n",
      "Epoch 0, Step 13200, Loss: 0.886609673500061\n",
      "Epoch 0, Step 13300, Loss: 0.9703425168991089\n",
      "Epoch 0, Step 13400, Loss: 1.1049070358276367\n",
      "Epoch 0, Step 13500, Loss: 1.9915456771850586\n",
      "Epoch 0, Step 13600, Loss: 0.7113295197486877\n",
      "Epoch 0, Step 13700, Loss: 1.4280167818069458\n",
      "Epoch 0, Step 13800, Loss: 0.8710917234420776\n",
      "Epoch 0, Step 13900, Loss: 0.3340863287448883\n",
      "Epoch 0, Step 14000, Loss: 1.7084676027297974\n",
      "Epoch 0, Step 14100, Loss: 0.271299809217453\n",
      "Epoch 0, Step 14200, Loss: 0.7507126331329346\n",
      "Epoch 0, Step 14300, Loss: 1.2050838470458984\n",
      "Epoch 0, Step 14400, Loss: 0.9769236445426941\n",
      "Epoch 0, Step 14500, Loss: 0.7311807870864868\n",
      "Epoch 0, Step 14600, Loss: 2.1557066440582275\n",
      "Epoch 0, Step 14700, Loss: 0.45418915152549744\n",
      "Epoch 0, Step 14800, Loss: 0.5040712952613831\n",
      "Epoch 0, Step 14900, Loss: 0.4906996488571167\n",
      "Epoch 0, Step 15000, Loss: 0.12295815348625183\n",
      "Epoch 0, Step 15100, Loss: 1.397735595703125\n",
      "Epoch 0, Step 15200, Loss: 0.8983957767486572\n",
      "Epoch 0, Step 15300, Loss: 0.8965618014335632\n",
      "Epoch 0, Step 15400, Loss: 0.6777138710021973\n",
      "Epoch 0, Step 15500, Loss: 0.6305975914001465\n",
      "Epoch 0, Step 15600, Loss: 1.4735314846038818\n",
      "Epoch 0, Step 15700, Loss: 0.9702758193016052\n",
      "Epoch 0, Step 15800, Loss: 0.6510839462280273\n",
      "Epoch 0, Step 15900, Loss: 0.6869874596595764\n",
      "Epoch 0, Step 16000, Loss: 1.4684027433395386\n",
      "Epoch 0, Step 16100, Loss: 0.7482527494430542\n",
      "Epoch 0, Step 16200, Loss: 0.6419967412948608\n",
      "Epoch 0, Step 16300, Loss: 1.0401318073272705\n",
      "Epoch 0, Step 16400, Loss: 0.7890986800193787\n",
      "Epoch 0, Step 16500, Loss: 0.8333208560943604\n",
      "Epoch 0, Step 16600, Loss: 0.9298145771026611\n",
      "Epoch 0, Step 16700, Loss: 0.8481899499893188\n",
      "Epoch 0, Step 16800, Loss: 0.6669860482215881\n",
      "Epoch 0, Step 16900, Loss: 1.286644458770752\n",
      "Epoch 0, Step 17000, Loss: 0.8210121393203735\n",
      "Epoch 0, Step 17100, Loss: 1.239238977432251\n",
      "Epoch 0, Step 17200, Loss: 0.8423399925231934\n",
      "Epoch 0, Step 17300, Loss: 0.44729018211364746\n",
      "Epoch 0, Step 17400, Loss: 1.3582366704940796\n",
      "Epoch 0, Step 17500, Loss: 2.2131540775299072\n",
      "Epoch 0, Step 17600, Loss: 0.6255990266799927\n",
      "Epoch 0, Step 17700, Loss: 0.8055305480957031\n",
      "Epoch 0, Step 17800, Loss: 0.8730273246765137\n",
      "Epoch 0, Step 17900, Loss: 0.9927874207496643\n",
      "Epoch 0, Step 18000, Loss: 2.2538421154022217\n",
      "Epoch 0, Step 18100, Loss: 1.599708914756775\n",
      "Epoch 0, Step 18200, Loss: 1.5507044792175293\n",
      "Epoch 0, Step 18300, Loss: 1.4145878553390503\n",
      "Epoch 0, Step 18400, Loss: 0.7230265140533447\n",
      "Epoch 0, Step 18500, Loss: 0.37192079424858093\n",
      "Epoch 0, Step 18600, Loss: 0.4449181854724884\n",
      "Epoch 0, Step 18700, Loss: 1.261206030845642\n",
      "Epoch 0, Step 18800, Loss: 1.1975756883621216\n",
      "Epoch 0, Step 18900, Loss: 0.42606496810913086\n",
      "Epoch 0, Step 19000, Loss: 1.2475560903549194\n",
      "Epoch 0, Step 19100, Loss: 0.6518560647964478\n",
      "Epoch 0, Step 19200, Loss: 1.218339443206787\n",
      "Epoch 0, Step 19300, Loss: 0.5596983432769775\n",
      "Epoch 0, Step 19400, Loss: 0.6301413774490356\n",
      "Epoch 0, Step 19500, Loss: 0.9841232299804688\n",
      "Epoch 0, Step 19600, Loss: 0.5535650253295898\n",
      "Epoch 0, Step 19700, Loss: 0.6950761079788208\n",
      "Epoch 0, Step 19800, Loss: 1.465099811553955\n",
      "Epoch 0, Step 19900, Loss: 1.4310014247894287\n",
      "Epoch 0, Step 20000, Loss: 2.7531940937042236\n",
      "Epoch 0, Step 20100, Loss: 0.6924093961715698\n",
      "Epoch 0, Step 20200, Loss: 1.141753077507019\n",
      "Epoch 0, Step 20300, Loss: 0.3023296594619751\n",
      "Epoch 0, Step 20400, Loss: 1.4230509996414185\n",
      "Epoch 0, Step 20500, Loss: 0.7890594601631165\n",
      "Epoch 0, Step 20600, Loss: 1.8814568519592285\n",
      "Epoch 0, Step 20700, Loss: 2.3397984504699707\n",
      "Epoch 0, Step 20800, Loss: 1.1781667470932007\n",
      "Epoch 0, Step 20900, Loss: 0.9026415348052979\n",
      "Epoch 0, Step 21000, Loss: 0.458578884601593\n",
      "Epoch 0, Step 21100, Loss: 1.714112401008606\n",
      "Epoch 0, Step 21200, Loss: 0.7930859327316284\n",
      "Epoch 0, Step 21300, Loss: 2.205411434173584\n",
      "Epoch 0, Step 21400, Loss: 1.1843112707138062\n",
      "Epoch 1, Step 0, Loss: 0.6179189085960388\n",
      "Epoch 1, Step 100, Loss: 0.6462622284889221\n",
      "Epoch 1, Step 200, Loss: 0.19854950904846191\n",
      "Epoch 1, Step 300, Loss: 0.9298394322395325\n",
      "Epoch 1, Step 400, Loss: 0.582741916179657\n",
      "Epoch 1, Step 500, Loss: 0.6836880445480347\n",
      "Epoch 1, Step 600, Loss: 0.7321104407310486\n",
      "Epoch 1, Step 700, Loss: 0.6914660930633545\n",
      "Epoch 1, Step 800, Loss: 1.8769012689590454\n",
      "Epoch 1, Step 900, Loss: 0.3244869112968445\n",
      "Epoch 1, Step 1000, Loss: 0.9977990388870239\n",
      "Epoch 1, Step 1100, Loss: 1.1029335260391235\n",
      "Epoch 1, Step 1200, Loss: 1.6652722358703613\n",
      "Epoch 1, Step 1300, Loss: 1.2549386024475098\n",
      "Epoch 1, Step 1400, Loss: 0.6109448075294495\n",
      "Epoch 1, Step 1500, Loss: 1.0648740530014038\n",
      "Epoch 1, Step 1600, Loss: 1.1102949380874634\n",
      "Epoch 1, Step 1700, Loss: 0.3969394564628601\n",
      "Epoch 1, Step 1800, Loss: 0.463203102350235\n",
      "Epoch 1, Step 1900, Loss: 1.0745513439178467\n",
      "Epoch 1, Step 2000, Loss: 0.9518015384674072\n",
      "Epoch 1, Step 2100, Loss: 0.9422505497932434\n",
      "Epoch 1, Step 2200, Loss: 1.4113149642944336\n",
      "Epoch 1, Step 2300, Loss: 0.9867604374885559\n",
      "Epoch 1, Step 2400, Loss: 1.0520166158676147\n",
      "Epoch 1, Step 2500, Loss: 1.2078678607940674\n",
      "Epoch 1, Step 2600, Loss: 0.5918590426445007\n",
      "Epoch 1, Step 2700, Loss: 0.7146202325820923\n",
      "Epoch 1, Step 2800, Loss: 0.8657732605934143\n",
      "Epoch 1, Step 2900, Loss: 0.9030253291130066\n",
      "Epoch 1, Step 3000, Loss: 1.0235460996627808\n",
      "Epoch 1, Step 3100, Loss: 1.0742040872573853\n",
      "Epoch 1, Step 3200, Loss: 1.13994562625885\n",
      "Epoch 1, Step 3300, Loss: 0.29039424657821655\n",
      "Epoch 1, Step 3400, Loss: 1.1251394748687744\n",
      "Epoch 1, Step 3500, Loss: 0.8151525855064392\n",
      "Epoch 1, Step 3600, Loss: 1.7791284322738647\n",
      "Epoch 1, Step 3700, Loss: 0.7892938852310181\n",
      "Epoch 1, Step 3800, Loss: 0.35937464237213135\n",
      "Epoch 1, Step 3900, Loss: 0.5099993348121643\n",
      "Epoch 1, Step 4000, Loss: 1.5241254568099976\n",
      "Epoch 1, Step 4100, Loss: 1.0412342548370361\n",
      "Epoch 1, Step 4200, Loss: 0.06396055221557617\n",
      "Epoch 1, Step 4300, Loss: 0.8424609899520874\n",
      "Epoch 1, Step 4400, Loss: 0.8594480156898499\n",
      "Epoch 1, Step 4500, Loss: 0.6214671730995178\n",
      "Epoch 1, Step 4600, Loss: 0.4864358901977539\n",
      "Epoch 1, Step 4700, Loss: 0.21623429656028748\n",
      "Epoch 1, Step 4800, Loss: 0.48879632353782654\n",
      "Epoch 1, Step 4900, Loss: 0.5178559422492981\n",
      "Epoch 1, Step 5000, Loss: 1.403450846672058\n",
      "Epoch 1, Step 5100, Loss: 0.5527079701423645\n",
      "Epoch 1, Step 5200, Loss: 0.5968451499938965\n",
      "Epoch 1, Step 5300, Loss: 1.5997891426086426\n",
      "Epoch 1, Step 5400, Loss: 0.36622878909111023\n",
      "Epoch 1, Step 5500, Loss: 1.202876091003418\n",
      "Epoch 1, Step 5600, Loss: 0.7578809261322021\n",
      "Epoch 1, Step 5700, Loss: 0.16676761209964752\n",
      "Epoch 1, Step 5800, Loss: 1.6199978590011597\n",
      "Epoch 1, Step 5900, Loss: 0.252026230096817\n",
      "Epoch 1, Step 6000, Loss: 0.5260694026947021\n",
      "Epoch 1, Step 6100, Loss: 0.46826106309890747\n",
      "Epoch 1, Step 6200, Loss: 0.8487058281898499\n",
      "Epoch 1, Step 6300, Loss: 0.41408297419548035\n",
      "Epoch 1, Step 6400, Loss: 1.6709697246551514\n",
      "Epoch 1, Step 6500, Loss: 1.0134360790252686\n",
      "Epoch 1, Step 6600, Loss: 0.6376678943634033\n",
      "Epoch 1, Step 6700, Loss: 1.3010952472686768\n",
      "Epoch 1, Step 6800, Loss: 0.40642890334129333\n",
      "Epoch 1, Step 6900, Loss: 1.7719807624816895\n",
      "Epoch 1, Step 7000, Loss: 1.7424848079681396\n",
      "Epoch 1, Step 7100, Loss: 0.21482324600219727\n",
      "Epoch 1, Step 7200, Loss: 1.9942188262939453\n",
      "Epoch 1, Step 7300, Loss: 0.11155223846435547\n",
      "Epoch 1, Step 7400, Loss: 0.6235228180885315\n",
      "Epoch 1, Step 7500, Loss: 1.7486987113952637\n",
      "Epoch 1, Step 7600, Loss: 0.1880887746810913\n",
      "Epoch 1, Step 7700, Loss: 1.4432858228683472\n",
      "Epoch 1, Step 7800, Loss: 0.525576114654541\n",
      "Epoch 1, Step 7900, Loss: 0.8372427821159363\n",
      "Epoch 1, Step 8000, Loss: 0.9278277158737183\n",
      "Epoch 1, Step 8100, Loss: 0.8907206654548645\n",
      "Epoch 1, Step 8200, Loss: 0.29997506737709045\n",
      "Epoch 1, Step 8300, Loss: 0.2600107789039612\n",
      "Epoch 1, Step 8400, Loss: 1.0499837398529053\n",
      "Epoch 1, Step 8500, Loss: 1.4937500953674316\n",
      "Epoch 1, Step 8600, Loss: 0.5263429880142212\n",
      "Epoch 1, Step 8700, Loss: 1.5584841966629028\n",
      "Epoch 1, Step 8800, Loss: 0.39154672622680664\n",
      "Epoch 1, Step 8900, Loss: 0.8017180562019348\n",
      "Epoch 1, Step 9000, Loss: 1.3892054557800293\n",
      "Epoch 1, Step 9100, Loss: 1.5360453128814697\n",
      "Epoch 1, Step 9200, Loss: 0.8691267371177673\n",
      "Epoch 1, Step 9300, Loss: 1.46002197265625\n",
      "Epoch 1, Step 9400, Loss: 0.3475152254104614\n",
      "Epoch 1, Step 9500, Loss: 0.8938369154930115\n",
      "Epoch 1, Step 9600, Loss: 1.077458143234253\n",
      "Epoch 1, Step 9700, Loss: 0.8942801356315613\n",
      "Epoch 1, Step 9800, Loss: 0.7524039149284363\n",
      "Epoch 1, Step 9900, Loss: 0.279803067445755\n",
      "Epoch 1, Step 10000, Loss: 0.62823885679245\n",
      "Epoch 1, Step 10100, Loss: 0.6018452048301697\n",
      "Epoch 1, Step 10200, Loss: 0.20996235311031342\n",
      "Epoch 1, Step 10300, Loss: 1.0516453981399536\n",
      "Epoch 1, Step 10400, Loss: 0.6425990462303162\n",
      "Epoch 1, Step 10500, Loss: 0.4309726655483246\n",
      "Epoch 1, Step 10600, Loss: 1.6256059408187866\n",
      "Epoch 1, Step 10700, Loss: 0.6633777022361755\n",
      "Epoch 1, Step 10800, Loss: 0.5353535413742065\n",
      "Epoch 1, Step 10900, Loss: 0.5009192824363708\n",
      "Epoch 1, Step 11000, Loss: 0.16811464726924896\n",
      "Epoch 1, Step 11100, Loss: 0.7641634941101074\n",
      "Epoch 1, Step 11200, Loss: 1.4076818227767944\n",
      "Epoch 1, Step 11300, Loss: 0.9749675393104553\n",
      "Epoch 1, Step 11400, Loss: 0.5862337350845337\n",
      "Epoch 1, Step 11500, Loss: 0.9815495610237122\n",
      "Epoch 1, Step 11600, Loss: 1.6888784170150757\n",
      "Epoch 1, Step 11700, Loss: 0.35015785694122314\n",
      "Epoch 1, Step 11800, Loss: 0.4088260233402252\n",
      "Epoch 1, Step 11900, Loss: 1.6845680475234985\n",
      "Epoch 1, Step 12000, Loss: 0.7443896532058716\n",
      "Epoch 1, Step 12100, Loss: 0.39737945795059204\n",
      "Epoch 1, Step 12200, Loss: 0.6258796453475952\n",
      "Epoch 1, Step 12300, Loss: 0.7461318969726562\n",
      "Epoch 1, Step 12400, Loss: 1.1373265981674194\n",
      "Epoch 1, Step 12500, Loss: 1.7011886835098267\n",
      "Epoch 1, Step 12600, Loss: 0.7307589054107666\n",
      "Epoch 1, Step 12700, Loss: 1.4501982927322388\n",
      "Epoch 1, Step 12800, Loss: 0.2705276906490326\n",
      "Epoch 1, Step 12900, Loss: 1.1938072443008423\n",
      "Epoch 1, Step 13000, Loss: 0.9471418261528015\n",
      "Epoch 1, Step 13100, Loss: 0.8239316940307617\n",
      "Epoch 1, Step 13200, Loss: 0.6567322611808777\n",
      "Epoch 1, Step 13300, Loss: 0.7354205846786499\n",
      "Epoch 1, Step 13400, Loss: 0.7015570402145386\n",
      "Epoch 1, Step 13500, Loss: 1.6748460531234741\n",
      "Epoch 1, Step 13600, Loss: 0.5961102843284607\n",
      "Epoch 1, Step 13700, Loss: 1.1270114183425903\n",
      "Epoch 1, Step 13800, Loss: 0.7128901481628418\n",
      "Epoch 1, Step 13900, Loss: 0.2638237476348877\n",
      "Epoch 1, Step 14000, Loss: 1.3139564990997314\n",
      "Epoch 1, Step 14100, Loss: 0.23894327878952026\n",
      "Epoch 1, Step 14200, Loss: 0.5519445538520813\n",
      "Epoch 1, Step 14300, Loss: 0.930120050907135\n",
      "Epoch 1, Step 14400, Loss: 0.851377010345459\n",
      "Epoch 1, Step 14500, Loss: 0.512842059135437\n",
      "Epoch 1, Step 14600, Loss: 1.6606425046920776\n",
      "Epoch 1, Step 14700, Loss: 0.3969581425189972\n",
      "Epoch 1, Step 14800, Loss: 0.3971075713634491\n",
      "Epoch 1, Step 14900, Loss: 0.3716498017311096\n",
      "Epoch 1, Step 15000, Loss: 0.07884912192821503\n",
      "Epoch 1, Step 15100, Loss: 1.1122573614120483\n",
      "Epoch 1, Step 15200, Loss: 0.6939646601676941\n",
      "Epoch 1, Step 15300, Loss: 0.7682918310165405\n",
      "Epoch 1, Step 15400, Loss: 0.4488039016723633\n",
      "Epoch 1, Step 15500, Loss: 0.3447476029396057\n",
      "Epoch 1, Step 15600, Loss: 1.1581228971481323\n",
      "Epoch 1, Step 15700, Loss: 0.7811830043792725\n",
      "Epoch 1, Step 15800, Loss: 0.5341516733169556\n",
      "Epoch 1, Step 15900, Loss: 0.5481688380241394\n",
      "Epoch 1, Step 16000, Loss: 1.0576069355010986\n",
      "Epoch 1, Step 16100, Loss: 0.5234873294830322\n",
      "Epoch 1, Step 16200, Loss: 0.5062985420227051\n",
      "Epoch 1, Step 16300, Loss: 0.7337713837623596\n",
      "Epoch 1, Step 16400, Loss: 0.5844500660896301\n",
      "Epoch 1, Step 16500, Loss: 0.6214672923088074\n",
      "Epoch 1, Step 16600, Loss: 0.7167326807975769\n",
      "Epoch 1, Step 16700, Loss: 0.6711493730545044\n",
      "Epoch 1, Step 16800, Loss: 0.42015540599823\n",
      "Epoch 1, Step 16900, Loss: 1.0991425514221191\n",
      "Epoch 1, Step 17000, Loss: 0.6389567852020264\n",
      "Epoch 1, Step 17100, Loss: 0.8688765168190002\n",
      "Epoch 1, Step 17200, Loss: 0.7438952922821045\n",
      "Epoch 1, Step 17300, Loss: 0.31468528509140015\n",
      "Epoch 1, Step 17400, Loss: 0.9846178293228149\n",
      "Epoch 1, Step 17500, Loss: 1.6058164834976196\n",
      "Epoch 1, Step 17600, Loss: 0.45674625039100647\n",
      "Epoch 1, Step 17700, Loss: 0.6739961504936218\n",
      "Epoch 1, Step 17800, Loss: 0.6629878878593445\n",
      "Epoch 1, Step 17900, Loss: 0.7069180607795715\n",
      "Epoch 1, Step 18000, Loss: 1.546047568321228\n",
      "Epoch 1, Step 18100, Loss: 1.3154833316802979\n",
      "Epoch 1, Step 18200, Loss: 1.2371104955673218\n",
      "Epoch 1, Step 18300, Loss: 1.197139024734497\n",
      "Epoch 1, Step 18400, Loss: 0.5902020335197449\n",
      "Epoch 1, Step 18500, Loss: 0.34740495681762695\n",
      "Epoch 1, Step 18600, Loss: 0.339220255613327\n",
      "Epoch 1, Step 18700, Loss: 0.9463302493095398\n",
      "Epoch 1, Step 18800, Loss: 0.9996909499168396\n",
      "Epoch 1, Step 18900, Loss: 0.2037688046693802\n",
      "Epoch 1, Step 19000, Loss: 0.9716692566871643\n",
      "Epoch 1, Step 19100, Loss: 0.5108779668807983\n",
      "Epoch 1, Step 19200, Loss: 0.9439604878425598\n",
      "Epoch 1, Step 19300, Loss: 0.4399277865886688\n",
      "Epoch 1, Step 19400, Loss: 0.4925805628299713\n",
      "Epoch 1, Step 19500, Loss: 0.8168078660964966\n",
      "Epoch 1, Step 19600, Loss: 0.4589998424053192\n",
      "Epoch 1, Step 19700, Loss: 0.5095020532608032\n",
      "Epoch 1, Step 19800, Loss: 1.0652744770050049\n",
      "Epoch 1, Step 19900, Loss: 1.1151788234710693\n",
      "Epoch 1, Step 20000, Loss: 2.328218936920166\n",
      "Epoch 1, Step 20100, Loss: 0.5349714159965515\n",
      "Epoch 1, Step 20200, Loss: 0.8990628719329834\n",
      "Epoch 1, Step 20300, Loss: 0.1788339614868164\n",
      "Epoch 1, Step 20400, Loss: 1.0796830654144287\n",
      "Epoch 1, Step 20500, Loss: 0.6405134797096252\n",
      "Epoch 1, Step 20600, Loss: 1.5531811714172363\n",
      "Epoch 1, Step 20700, Loss: 1.9850690364837646\n",
      "Epoch 1, Step 20800, Loss: 0.9865491390228271\n",
      "Epoch 1, Step 20900, Loss: 0.7072140574455261\n",
      "Epoch 1, Step 21000, Loss: 0.382754921913147\n",
      "Epoch 1, Step 21100, Loss: 1.3997368812561035\n",
      "Epoch 1, Step 21200, Loss: 0.6362431049346924\n",
      "Epoch 1, Step 21300, Loss: 1.7803398370742798\n",
      "Epoch 1, Step 21400, Loss: 0.8912972211837769\n",
      "Epoch 2, Step 0, Loss: 0.5265504717826843\n",
      "Epoch 2, Step 100, Loss: 0.5031441450119019\n",
      "Epoch 2, Step 200, Loss: 0.16285531222820282\n",
      "Epoch 2, Step 300, Loss: 0.7665346264839172\n",
      "Epoch 2, Step 400, Loss: 0.37156161665916443\n",
      "Epoch 2, Step 500, Loss: 0.5008744597434998\n",
      "Epoch 2, Step 600, Loss: 0.44927743077278137\n",
      "Epoch 2, Step 700, Loss: 0.606974720954895\n",
      "Epoch 2, Step 800, Loss: 1.5374343395233154\n",
      "Epoch 2, Step 900, Loss: 0.13985446095466614\n",
      "Epoch 2, Step 1000, Loss: 0.8544926643371582\n",
      "Epoch 2, Step 1100, Loss: 0.9485528469085693\n",
      "Epoch 2, Step 1200, Loss: 1.3295915126800537\n",
      "Epoch 2, Step 1300, Loss: 1.0875306129455566\n",
      "Epoch 2, Step 1400, Loss: 0.48375403881073\n",
      "Epoch 2, Step 1500, Loss: 0.9350234270095825\n",
      "Epoch 2, Step 1600, Loss: 1.0400842428207397\n",
      "Epoch 2, Step 1700, Loss: 0.3188597857952118\n",
      "Epoch 2, Step 1800, Loss: 0.42739710211753845\n",
      "Epoch 2, Step 1900, Loss: 0.8689011931419373\n",
      "Epoch 2, Step 2000, Loss: 0.7729203104972839\n",
      "Epoch 2, Step 2100, Loss: 0.7208089828491211\n",
      "Epoch 2, Step 2200, Loss: 1.2483235597610474\n",
      "Epoch 2, Step 2300, Loss: 0.8667795062065125\n",
      "Epoch 2, Step 2400, Loss: 0.9445894360542297\n",
      "Epoch 2, Step 2500, Loss: 1.047704815864563\n",
      "Epoch 2, Step 2600, Loss: 0.46040692925453186\n",
      "Epoch 2, Step 2700, Loss: 0.5068846940994263\n",
      "Epoch 2, Step 2800, Loss: 0.6790132522583008\n",
      "Epoch 2, Step 2900, Loss: 0.7464995384216309\n",
      "Epoch 2, Step 3000, Loss: 0.7418694496154785\n",
      "Epoch 2, Step 3100, Loss: 1.0328736305236816\n",
      "Epoch 2, Step 3200, Loss: 0.9116718769073486\n",
      "Epoch 2, Step 3300, Loss: 0.26932060718536377\n",
      "Epoch 2, Step 3400, Loss: 0.9440842270851135\n",
      "Epoch 2, Step 3500, Loss: 0.704093873500824\n",
      "Epoch 2, Step 3600, Loss: 1.4595333337783813\n",
      "Epoch 2, Step 3700, Loss: 0.7247793078422546\n",
      "Epoch 2, Step 3800, Loss: 0.25304603576660156\n",
      "Epoch 2, Step 3900, Loss: 0.32446348667144775\n",
      "Epoch 2, Step 4000, Loss: 1.2614668607711792\n",
      "Epoch 2, Step 4100, Loss: 0.7900230884552002\n",
      "Epoch 2, Step 4200, Loss: 0.08558663725852966\n",
      "Epoch 2, Step 4300, Loss: 0.6954312920570374\n",
      "Epoch 2, Step 4400, Loss: 0.6876115202903748\n",
      "Epoch 2, Step 4500, Loss: 0.4762769937515259\n",
      "Epoch 2, Step 4600, Loss: 0.3879667818546295\n",
      "Epoch 2, Step 4700, Loss: 0.18839041888713837\n",
      "Epoch 2, Step 4800, Loss: 0.44240233302116394\n",
      "Epoch 2, Step 4900, Loss: 0.37041518092155457\n",
      "Epoch 2, Step 5000, Loss: 1.0911158323287964\n",
      "Epoch 2, Step 5100, Loss: 0.4524754285812378\n",
      "Epoch 2, Step 5200, Loss: 0.49554282426834106\n",
      "Epoch 2, Step 5300, Loss: 1.3213106393814087\n",
      "Epoch 2, Step 5400, Loss: 0.26993539929389954\n",
      "Epoch 2, Step 5500, Loss: 1.0015758275985718\n",
      "Epoch 2, Step 5600, Loss: 0.5603545308113098\n",
      "Epoch 2, Step 5700, Loss: 0.0906587466597557\n",
      "Epoch 2, Step 5800, Loss: 1.236817717552185\n",
      "Epoch 2, Step 5900, Loss: 0.24760569632053375\n",
      "Epoch 2, Step 6000, Loss: 0.43225255608558655\n",
      "Epoch 2, Step 6100, Loss: 0.46010300517082214\n",
      "Epoch 2, Step 6200, Loss: 0.6807867288589478\n",
      "Epoch 2, Step 6300, Loss: 0.2850571274757385\n",
      "Epoch 2, Step 6400, Loss: 1.4341291189193726\n",
      "Epoch 2, Step 6500, Loss: 0.8310858607292175\n",
      "Epoch 2, Step 6600, Loss: 0.44843682646751404\n",
      "Epoch 2, Step 6700, Loss: 1.0631035566329956\n",
      "Epoch 2, Step 6800, Loss: 0.3224183917045593\n",
      "Epoch 2, Step 6900, Loss: 1.4682259559631348\n",
      "Epoch 2, Step 7000, Loss: 1.4532577991485596\n",
      "Epoch 2, Step 7100, Loss: 0.10915559530258179\n",
      "Epoch 2, Step 7200, Loss: 1.5112650394439697\n",
      "Epoch 2, Step 7300, Loss: 0.08384357392787933\n",
      "Epoch 2, Step 7400, Loss: 0.49428054690361023\n",
      "Epoch 2, Step 7500, Loss: 1.4522722959518433\n",
      "Epoch 2, Step 7600, Loss: 0.13483470678329468\n",
      "Epoch 2, Step 7700, Loss: 1.0424128770828247\n",
      "Epoch 2, Step 7800, Loss: 0.36886343359947205\n",
      "Epoch 2, Step 7900, Loss: 0.5990335941314697\n",
      "Epoch 2, Step 8000, Loss: 0.8600296974182129\n",
      "Epoch 2, Step 8100, Loss: 0.6918089389801025\n",
      "Epoch 2, Step 8200, Loss: 0.20972219109535217\n",
      "Epoch 2, Step 8300, Loss: 0.2006528377532959\n",
      "Epoch 2, Step 8400, Loss: 0.8476166129112244\n",
      "Epoch 2, Step 8500, Loss: 1.2451485395431519\n",
      "Epoch 2, Step 8600, Loss: 0.375799298286438\n",
      "Epoch 2, Step 8700, Loss: 1.2625668048858643\n",
      "Epoch 2, Step 8800, Loss: 0.3543925881385803\n",
      "Epoch 2, Step 8900, Loss: 0.6026813983917236\n",
      "Epoch 2, Step 9000, Loss: 1.0574188232421875\n",
      "Epoch 2, Step 9100, Loss: 1.1748383045196533\n",
      "Epoch 2, Step 9200, Loss: 0.7072823643684387\n",
      "Epoch 2, Step 9300, Loss: 1.1747161149978638\n",
      "Epoch 2, Step 9400, Loss: 0.2882906198501587\n",
      "Epoch 2, Step 9500, Loss: 0.6299722194671631\n",
      "Epoch 2, Step 9600, Loss: 0.8013244271278381\n",
      "Epoch 2, Step 9700, Loss: 0.7301397919654846\n",
      "Epoch 2, Step 9800, Loss: 0.6876221299171448\n",
      "Epoch 2, Step 9900, Loss: 0.2333308756351471\n",
      "Epoch 2, Step 10000, Loss: 0.5527161359786987\n",
      "Epoch 2, Step 10100, Loss: 0.5182404518127441\n",
      "Epoch 2, Step 10200, Loss: 0.15801911056041718\n",
      "Epoch 2, Step 10300, Loss: 0.7500768899917603\n",
      "Epoch 2, Step 10400, Loss: 0.458965539932251\n",
      "Epoch 2, Step 10500, Loss: 0.28909003734588623\n",
      "Epoch 2, Step 10600, Loss: 1.2289118766784668\n",
      "Epoch 2, Step 10700, Loss: 0.45777809619903564\n",
      "Epoch 2, Step 10800, Loss: 0.3939781188964844\n",
      "Epoch 2, Step 10900, Loss: 0.35738566517829895\n",
      "Epoch 2, Step 11000, Loss: 0.09439373761415482\n",
      "Epoch 2, Step 11100, Loss: 0.5842560529708862\n",
      "Epoch 2, Step 11200, Loss: 1.0545756816864014\n",
      "Epoch 2, Step 11300, Loss: 0.8264634013175964\n",
      "Epoch 2, Step 11400, Loss: 0.4851968288421631\n",
      "Epoch 2, Step 11500, Loss: 0.7541543841362\n",
      "Epoch 2, Step 11600, Loss: 1.3476526737213135\n",
      "Epoch 2, Step 11700, Loss: 0.2860524356365204\n",
      "Epoch 2, Step 11800, Loss: 0.25718268752098083\n",
      "Epoch 2, Step 11900, Loss: 1.45189368724823\n",
      "Epoch 2, Step 12000, Loss: 0.5871508717536926\n",
      "Epoch 2, Step 12100, Loss: 0.4583873748779297\n",
      "Epoch 2, Step 12200, Loss: 0.5281869769096375\n",
      "Epoch 2, Step 12300, Loss: 0.5745128393173218\n",
      "Epoch 2, Step 12400, Loss: 0.9351500868797302\n",
      "Epoch 2, Step 12500, Loss: 1.204047679901123\n",
      "Epoch 2, Step 12600, Loss: 0.5851558446884155\n",
      "Epoch 2, Step 12700, Loss: 1.323078989982605\n",
      "Epoch 2, Step 12800, Loss: 0.22773262858390808\n",
      "Epoch 2, Step 12900, Loss: 1.0075236558914185\n",
      "Epoch 2, Step 13000, Loss: 0.7136889100074768\n",
      "Epoch 2, Step 13100, Loss: 0.6338902711868286\n",
      "Epoch 2, Step 13200, Loss: 0.39896970987319946\n",
      "Epoch 2, Step 13300, Loss: 0.4336912930011749\n",
      "Epoch 2, Step 13400, Loss: 0.5071726441383362\n",
      "Epoch 2, Step 13500, Loss: 1.2357714176177979\n",
      "Epoch 2, Step 13600, Loss: 0.5361596941947937\n",
      "Epoch 2, Step 13700, Loss: 0.997369647026062\n",
      "Epoch 2, Step 13800, Loss: 0.6069490909576416\n",
      "Epoch 2, Step 13900, Loss: 0.19199970364570618\n",
      "Epoch 2, Step 14000, Loss: 0.9196648001670837\n",
      "Epoch 2, Step 14100, Loss: 0.1919502317905426\n",
      "Epoch 2, Step 14200, Loss: 0.4034668207168579\n",
      "Epoch 2, Step 14300, Loss: 0.70723956823349\n",
      "Epoch 2, Step 14400, Loss: 0.7316484451293945\n",
      "Epoch 2, Step 14500, Loss: 0.4537618160247803\n",
      "Epoch 2, Step 14600, Loss: 1.3829646110534668\n",
      "Epoch 2, Step 14700, Loss: 0.30724507570266724\n",
      "Epoch 2, Step 14800, Loss: 0.35119929909706116\n",
      "Epoch 2, Step 14900, Loss: 0.30198660492897034\n",
      "Epoch 2, Step 15000, Loss: 0.06822788715362549\n",
      "Epoch 2, Step 15100, Loss: 0.9832717776298523\n",
      "Epoch 2, Step 15200, Loss: 0.4759489893913269\n",
      "Epoch 2, Step 15300, Loss: 0.5275329351425171\n",
      "Epoch 2, Step 15400, Loss: 0.3260279893875122\n",
      "Epoch 2, Step 15500, Loss: 0.27500689029693604\n",
      "Epoch 2, Step 15600, Loss: 0.9261330366134644\n",
      "Epoch 2, Step 15700, Loss: 0.5553874969482422\n",
      "Epoch 2, Step 15800, Loss: 0.4134064018726349\n",
      "Epoch 2, Step 15900, Loss: 0.5802550315856934\n",
      "Epoch 2, Step 16000, Loss: 0.7771852612495422\n",
      "Epoch 2, Step 16100, Loss: 0.45800870656967163\n",
      "Epoch 2, Step 16200, Loss: 0.3902176320552826\n",
      "Epoch 2, Step 16300, Loss: 0.671981930732727\n",
      "Epoch 2, Step 16400, Loss: 0.5231901407241821\n",
      "Epoch 2, Step 16500, Loss: 0.5626975893974304\n",
      "Epoch 2, Step 16600, Loss: 0.6485294699668884\n",
      "Epoch 2, Step 16700, Loss: 0.5445759296417236\n",
      "Epoch 2, Step 16800, Loss: 0.48263463377952576\n",
      "Epoch 2, Step 16900, Loss: 0.8742138147354126\n",
      "Epoch 2, Step 17000, Loss: 0.54067063331604\n",
      "Epoch 2, Step 17100, Loss: 0.6454983353614807\n",
      "Epoch 2, Step 17200, Loss: 0.6520142555236816\n",
      "Epoch 2, Step 17300, Loss: 0.23194541037082672\n",
      "Epoch 2, Step 17400, Loss: 0.7566390037536621\n",
      "Epoch 2, Step 17500, Loss: 1.1859875917434692\n",
      "Epoch 2, Step 17600, Loss: 0.370827317237854\n",
      "Epoch 2, Step 17700, Loss: 0.5084550976753235\n",
      "Epoch 2, Step 17800, Loss: 0.5577749609947205\n",
      "Epoch 2, Step 17900, Loss: 0.5660733580589294\n",
      "Epoch 2, Step 18000, Loss: 1.2445170879364014\n",
      "Epoch 2, Step 18100, Loss: 1.0535309314727783\n",
      "Epoch 2, Step 18200, Loss: 0.9512625932693481\n",
      "Epoch 2, Step 18300, Loss: 0.874076247215271\n",
      "Epoch 2, Step 18400, Loss: 0.4763505458831787\n",
      "Epoch 2, Step 18500, Loss: 0.3160000145435333\n",
      "Epoch 2, Step 18600, Loss: 0.32551610469818115\n",
      "Epoch 2, Step 18700, Loss: 0.6304082870483398\n",
      "Epoch 2, Step 18800, Loss: 0.8161978721618652\n",
      "Epoch 2, Step 18900, Loss: 0.15176619589328766\n",
      "Epoch 2, Step 19000, Loss: 0.7412067651748657\n",
      "Epoch 2, Step 19100, Loss: 0.43813520669937134\n",
      "Epoch 2, Step 19200, Loss: 0.7431765198707581\n",
      "Epoch 2, Step 19300, Loss: 0.33204758167266846\n",
      "Epoch 2, Step 19400, Loss: 0.40815407037734985\n",
      "Epoch 2, Step 19500, Loss: 0.6260465979576111\n",
      "Epoch 2, Step 19600, Loss: 0.33457234501838684\n",
      "Epoch 2, Step 19700, Loss: 0.44760116934776306\n",
      "Epoch 2, Step 19800, Loss: 0.8333879709243774\n",
      "Epoch 2, Step 19900, Loss: 0.9014754891395569\n",
      "Epoch 2, Step 20000, Loss: 1.7389370203018188\n",
      "Epoch 2, Step 20100, Loss: 0.46440452337265015\n",
      "Epoch 2, Step 20200, Loss: 0.6974111199378967\n",
      "Epoch 2, Step 20300, Loss: 0.13847589492797852\n",
      "Epoch 2, Step 20400, Loss: 0.9206632375717163\n",
      "Epoch 2, Step 20500, Loss: 0.602240800857544\n",
      "Epoch 2, Step 20600, Loss: 1.3529102802276611\n",
      "Epoch 2, Step 20700, Loss: 1.5049444437026978\n",
      "Epoch 2, Step 20800, Loss: 0.7603800892829895\n",
      "Epoch 2, Step 20900, Loss: 0.5488486886024475\n",
      "Epoch 2, Step 21000, Loss: 0.3151148557662964\n",
      "Epoch 2, Step 21100, Loss: 1.0815123319625854\n",
      "Epoch 2, Step 21200, Loss: 0.4690961241722107\n",
      "Epoch 2, Step 21300, Loss: 1.4584629535675049\n",
      "Epoch 2, Step 21400, Loss: 0.8261526226997375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_train_epochs = 3\n",
    "logging_steps = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for i in range(len(train_texts)):\n",
    "        input_text = train_texts[i]\n",
    "        input_encoding = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input_ids = input_encoding.input_ids.to(model.device)\n",
    "        \n",
    "        target_summary = train_summaries[i]\n",
    "        target_encoding = tokenizer(target_summary, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        labels = target_encoding.input_ids.to(model.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % logging_steps == 0:\n",
    "            print(f\"Epoch {epoch}, Step {i}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model\\config.json\n",
      "Model weights saved in trained_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the model if needed\n",
    "model.save_pretrained(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le modèle BART pour la génération de texte\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "for i in range(len(val_texts)):\n",
    "    input_text = val_texts[i]\n",
    "    input_encoding = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = input_encoding.input_ids.to(model.device)\n",
    "    \n",
    "    # Generate summary\n",
    "    output_ids = model.generate(input_ids)\n",
    "    output_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE score\n",
    "    rouge_scores = scorer.score(output_summary, val_summaries[i])\n",
    "    rouge_l_fmeasure = rouge_scores['rougeL'].fmeasure\n",
    "    \n",
    "    print(f\"Example {i+1}, ROUGE-L F-Score: {rouge_l_fmeasure}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/test_text.csv\")\n",
    "model=model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 4  \n",
    "num_batches = (len(test_data) + batch_size - 1) // batch_size\n",
    "\n",
    "generated_summaries = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_data = test_data.iloc[i * batch_size: (i + 1) * batch_size]\n",
    "    \n",
    "    test_encodings = tokenizer(batch_data[\"text\"].tolist(), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=test_encodings[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    decoded_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    generated_summaries.extend(decoded_summaries)\n",
    "\n",
    "submission_df = pd.DataFrame({\"ID\": test_data[\"ID\"], \"titles\": generated_summaries})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
